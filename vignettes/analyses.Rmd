---
title: "COVID-19 Stuff"
author: "My Heart Fitness"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


#### Basic questions:
* Seek the patients' perspectives around the importance of exercising during pandemic.
* Would also like to know whether they perceive their physicians to prioritize exercise and lifestyle behaviours any less importantly than they otherwise might, during this pandemic.

#### Variables and interactions of interest:
1. MET minutes
1. Volume of visits
1. Attendance
1. % Attendance and Met-minute interactions (because it is a sign of compliance)
1. Mean age and % females.

Use a time series analysis. We should do some p values using time series comparing pre-post trends around the week of march 16th within each year. We also want to compare time trends in 2020 vs 2019 statistically.

## Investigating Pre/post COVID restrictions 2020 in *MET-minutes*:

We'll start by using `read_data()` to create the dataset we're going to use and properly format/type the attributes. This is more an exercise in data wrangling than anything else.
We need to ascertain the model we're going to be using, and learn about the data and its features. The data runs from Jan 1 2019 until April 31 2019 and Jan 1 2019 until April 31, 2020. We begin by asking if the time series for 2019 and 2020 are stationary. Let's start with a visual inspection. We'll plot the variable against time. For *MET-minutes*, our plots for 2019 and 2020 are below.

```{r}
# Met-minutes in 2019
plot(ts(RMHF::read_data()[1:11,"Met-minutes"]))
#  Met minutes in 2020
plot(ts(RMHF::read_data()[2:22,"Met-minutes"]))
```

_We also note that there is no consistent trend in either time series over the entire time span. The series appear to wander up and down. There also don't appear to be any obvious outliers in either time series._

First, let's use an augmented Dickey-Fuller test to determine whether a time series is stationary. Technically, we're going to test the null hypothesis that the series is non-stationary.

```{r}
df <- RMHF::read_data()
# Is the 2019 time-series stationary?
df2019mets <- df[1:11, "Met-minutes"]
tsdf2019mets <- ts(df2019mets)
tseries::adf.test(tsdf2019mets, alternative = "stationary")
# Is the 2020 time-series stationary?
df2020mets <- df[11:22, "Met-minutes"]
tsdf2020mets <- ts(df2020mets)
tseries::adf.test(tsdf2020mets, alternative = "stationary")
```

Alright, so we can't reject the null hypothesis that the series is non-stationary in either 2019 or 2020. This is important for us. Given that we have non-stationary data, we will need to “difference” the data until we obtain a stationary time series. We'll start with the “first-order"" difference. What we're doing here is that is removing the previous Y _met-minutes_ values only once.  For each time point in our data, this gives you the change in value from the previous time point. So let's do this in parallel for 2019 and 2020.

```{r}
# Differencing 2019 data.
tsdf2019mets_diff1<- diff(tsdf2019mets, differences=1)
tseries::adf.test(tsdf2019mets_diff1, alternative = "stationary")

# Differencing 2020 data.
tsdf2020mets_diff1 <- diff(tsdf2020mets, differences=1)
tseries::adf.test(tsdf2020mets_diff1, alternative = "stationary")
```

Differencing the first time didn't make either time series stationary. Let's try second order differencing.

```{r}
# Second differencing 2019 data.
tsdf2019mets_diff2<- diff(tsdf2019mets, differences=2)
tseries::adf.test(tsdf2019mets_diff2, alternative = "stationary")

# Second differencing 2020 data.
tsdf2020mets_diff2 <- diff(tsdf2020mets, differences=2)
tseries::adf.test(tsdf2020mets_diff2, alternative = "stationary")
```

Differencing the second time didn't make either time series stationary. Let's try third order differencing.

```{r}
# Third differencing 2019 data.
tsdf2019mets_diff3 <- diff(tsdf2019mets, differences=3)
tseries::adf.test(tsdf2019mets_diff3, alternative = "stationary")

# Third differencing 2020 data.
tsdf2020mets_diff3 <- diff(tsdf2020mets, differences=3)
tseries::adf.test(tsdf2020mets_diff3, alternative = "stationary")
```

It looks like our 2019 time series is stationary now. We can reject the null hypothesis that it isn't stationary and conclude that it is now. Let's difference one more time to make our 2020 data stationary.
```{r}
# Fourth differencing 2020 data.
tsdf2020mets_diff4 <- diff(tsdf2020mets, differences=4)
tseries::adf.test(tsdf2020mets_diff4, alternative = "stationary")
```
Alright, now both time series objects are stationary. Let's take a look at the graphs of both now. Note that the differencing consumed some data points at the begining and end of each time series.
```{r}
# Third order differenced 2019 data.
plot(tsdf2019mets_diff3)
# Fourth order differenced 2020 data.
plot(tsdf2020mets_diff4)
```

The d values for 2019 and 2020 arima models are therefore 3 and 4, respectively. Now that we have a stationary time series for each year, we should select the appropriate ARIMA model. This means finding the most appropriate values for p and q in the ARIMA(p,d,q) model.

As a quick refresher, p refers to how many previous (lagged) *Y* values are accounted for, and q refers to how many previous (lagged) *error* values are accounted for each time point in our model. We'll use two tools here:
1. Correlogram
1. Correlogram partial correlogram

What's a correlogram? It's a plot of the correlation of each MET-minute variable at time t with that at time t-k. A partial correlogram the same except for the fact that it removes the effect of shorter autocorrelation lags when calculating the correlation at longer lags. Techhnically speaking, the partial correlation at lag k is the autocorrelation between Yt and Yt-k that is NOT accounted for by the autocorrelations from the 1st to the (k-1)st lags.

Let's explore the correlogram and partial correlograms for the 2019 and 2020 data, respectively.
```{r}
# Correlogram for stationary (third order) differenced 2019 data.
acf(as.numeric(tsdf2019mets_diff3))
# Partial correlogram for stationary (third order) differenced 2019 data.
pacf(as.numeric(tsdf2019mets_diff3))


# Correlogram for stationary (fourth order) differenced 2020 data.
acf(tsdf2020mets_diff4)
# Partial correlogram for stationary (third order) differenced 2020 data.
pacf(tsdf2020mets_diff4)

```

Alright, there were some significant autocorrelations at the begining of each time series, so let's extract the dominant frequency from each time series objet.
```{r}
#Adpot constant naming conventions.
stat2019 <- tsdf2019mets_diff3
stat2020 <- tsdf2020mets_diff4
#Fast fourier transform for 2019 data.
forecast::findfrequency(stat2019)
#Fast fourier transform for 2020.
forecast::findfrequency(stat2020)
```
Alright, so the dominant frequency in each dataset appears to be three. Let's fit some time series objects.
```{r}
#Fit the appropraite time series for 2019 data.
ts2019 <-  ts(stat2019[1:8], frequency = 3)
#Fit the apropriate time series for 2020 data.
ts2020 <- ts(stat2020[1:7], frequency = 3)
```
Now we can use local regression modelling to decompose each time series object. Lets do that and explore each object.
```{r}
#Decompose 2019 data.
stl_ts2019 <-  stl(ts2019, s.window = "periodic")
plot(stl_ts2019)

#Decompose 2020 data.
stl_ts2020 <-  stl(ts2020, s.window = "periodic")
plot(stl_ts2020)

```
Now we can use seasonal adjustent to remove the seasonal component from each time series. Lets examine each as well.
```{r}
#Adjust 2019 data.
saeasadj2019 <-  forecast::seasadj(stl_ts2019)
plot(saeasadj2019)

#Adjust 2020 data.
saeasadj2020 <-  forecast::seasadj(stl_ts2020)
plot(saeasadj2020)
```

Now we explore at the autocorrelations and partial autocorrelations plots, we should see no patterns.
```{r}
#2019
acf(as.numeric(saeasadj2019))

#2020
acf(as.numeric(saeasadj2020))
```
Lags don't appear to be correlated to oneanother, which is good.
The maximum significant lag values of the correlogram gives the possible q values for the ARIMA model. For instance, if our maximum value is n, then an ARMA(0,n) model is possible.

The maximum significant lag values of the partial correlogram gives  the p value for an ARIMA model. For instance, if our maximum value is n, then an an ARMA(n,0) model would also be possible.




